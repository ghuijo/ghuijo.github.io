<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://ghuijo.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://ghuijo.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-10-09T22:01:40+00:00</updated><id>https://ghuijo.github.io/feed.xml</id><title type="html">blank</title><entry><title type="html">Reflections on My Early Weeks as a Visiting Researcher at FAU</title><link href="https://ghuijo.github.io/blog/2025/Reflections-FAU/" rel="alternate" type="text/html" title="Reflections on My Early Weeks as a Visiting Researcher at FAU"/><published>2025-10-09T00:00:00+00:00</published><updated>2025-10-09T00:00:00+00:00</updated><id>https://ghuijo.github.io/blog/2025/Reflections-FAU</id><content type="html" xml:base="https://ghuijo.github.io/blog/2025/Reflections-FAU/"><![CDATA[<p>Hello! Since late September, I’ve been staying in Germany as a visiting researcher at <strong>Friedrich-Alexander-Universität Erlangen-Nürnberg (FAU)</strong>, working on a project titled <em>Joint Research on the Development of AI-based Image Processing Algorithms</em>.</p> <p>It has already been about three weeks since I started my visit, and today I joined the <strong>Colloquium</strong> for the first time—one of the most interesting research traditions at PRL (Pattern Recognition Lab), FAU. In this post, I’d like to share some of the lab’s research culture and what has impressed me so far.</p> <h2 id="1-the-50th-anniversary-celebration">1. The 50th Anniversary Celebration</h2> <p>Right after arriving in Germany, I attended the lab’s <strong>50th-anniversary celebration</strong>, which was a major event.<br/> Coming from a Korean research culture—where a single PI typically leads the lab—it was fascinating to see a group that has been continuously active for <strong>five decades</strong>, now under its third chair.</p> <p>The two-day conference brought together around 400 participants, including professors, alumni, former members, and collaborators. It was an amazing opportunity to meet and talk with so many people across generations of the lab’s history.</p> <p>I was pleasantly surprised to reconnect with several German researchers who had previously visited our <strong>Medical AI &amp; Computer Vision Lab</strong> in Korea for collaboration. Being surrounded by hundreds of people sharing a common research heritage over half a century made me reflect deeply on the meaning of an academic community and continuity in research.</p> <h2 id="2-lunch-together-every-day">2. Lunch Together, Every Day</h2> <p>The <strong>PRL</strong> is quite a large lab—around 70 to 80 members in total. With so many people, you might expect everyone to stick to their own groups. But here, there’s a strong culture of having lunch together.</p> <p>Except for those stationed at partner hospitals or companies, members usually gather at the ground floor of the lab building and head out together to the <strong>Mensa</strong>, the university cafeteria. On the way there, and throughout lunch, research discussions never stop.</p> <p>Since the lab covers a wide range of topics—from medical imaging and computer vision to time series and signal processing—the conversations are always diverse and insightful. After lunch, people often take a short walk around the campus before returning to their desks.</p> <p>Recent topics included the <strong>MICCAI 2025 conference</strong> held in Korea, <strong>colloquium</strong>, ongoing projects, and even cycling or hiking plans (They really love biking!).<br/> The conversations flow naturally in <strong>English</strong>, as the group is truly international and open.</p> <h2 id="3-the-colloquium">3. The Colloquium</h2> <p>Every member of the lab belongs to at least one <strong>research group</strong>, such as <em>Computer Vision</em>, <em>LAMDA</em>, <em>IPA</em>, or <em>Time Series</em>.<br/> Each group holds a <strong>weekly colloquium</strong>, where members present their work or invite speakers.</p> <p>Today, I attended my first sessions—one for the <em>Time Series</em> group at 2 p.m. and another for <em>LAMDA</em> at 4 p.m.<br/> Typically, a Master’s student presents either an introduction or a final presentation of their thesis. After the presentation—covering background, methods, results, and future work—the entire group participates in discussion and Q&amp;A.</p> <p>For final presentations, PhD and post-doctoral researchers also provide formal evaluation and feedback. I was invited to join the evaluation myself, and, unfamiliar with the German grading system, I initially gave letter grades (A+, A, A-, etc.), which caused a small but funny misunderstanding before they kindly converted my input.</p> <p>The evaluation criteria are transparent and well-structured, and the process encourages rich discussion before reaching a collective decision. In addition to thesis presentations, the colloquium sometimes includes guest talks and PhD progress reports, so I’m looking forward to attending more sessions.</p> <h2 id="4-daily-routine-and-research-goals">4. Daily Routine and Research Goals</h2> <p>During this visit, I’ve been trying to make the most of my time through structured work and active collaboration.<br/> Most lab members start their day around <strong>8 a.m.</strong>, focusing deeply during work hours to enjoy evenings off—something I’m gradually adopting (though late-night work still happens occasionally).</p> <p>Everyone here seems highly motivated and efficient, balancing research, collaboration, and personal time remarkably well.<br/> As our collaboration progresses, I hope to share more about the <strong>joint research</strong> itself in my next post.</p> <hr/> <p><em>Thanks for reading! If you have any questions about the visiting research experience at FAU, feel free to contact me.</em></p>]]></content><author><name></name></author><category term="research"/><category term="collaboration"/><category term="Visiting"/><category term="Reseach,"/><category term="Computer"/><category term="Vision,"/><category term="FAU,"/><category term="Germany"/><summary type="html"><![CDATA[Hello! Since late September, I’ve been staying in Germany as a visiting researcher at Friedrich-Alexander-Universität Erlangen-Nürnberg (FAU), working on a project titled Joint Research on the Development of AI-based Image Processing Algorithms.]]></summary></entry><entry><title type="html">LeRobot Pi0 Finetuning Tutorial</title><link href="https://ghuijo.github.io/blog/2025/LeRobot-PI0-Finetuning-Tutorial/" rel="alternate" type="text/html" title="LeRobot Pi0 Finetuning Tutorial"/><published>2025-08-29T00:00:00+00:00</published><updated>2025-08-29T00:00:00+00:00</updated><id>https://ghuijo.github.io/blog/2025/LeRobot-PI0-Finetuning-Tutorial</id><content type="html" xml:base="https://ghuijo.github.io/blog/2025/LeRobot-PI0-Finetuning-Tutorial/"><![CDATA[<p>This is a blog post on how to finetune LeRobot Pi0 with SO-ARM101.</p> <h2 id="references">References</h2> <p><a href="https://huggingface.co/docs/lerobot/il_robots">LeRobot Imitation Learning Tutorial</a></p> <p><a href="https://huggingface.co/blog/pi0">LeRobot Pi0 and Pi0-fast Blog</a></p> <h2 id="experiment-environment">Experiment environment</h2> <ul> <li>Robot: SO-ARM101</li> <li>GPU: Tesla A100 SXM4 40GB</li> <li>OS: Ubuntu 20.04 LTS</li> <li>NVIDIA Driver: 570.169 (CUDA 12.8)</li> </ul> <h2 id="anaconda-environment-setup">Anaconda environment setup</h2> <ul> <li> <p><strong>Install Python 3.10 or higher</strong></p> </li> <li><strong>Install PyTorch</strong> Currently, LeRobot only supports PyTorch versions below 2.8. Use the following command to install a compatible version: <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span><span class="nv">torch</span><span class="o">==</span>2.7.1 <span class="nv">torchvision</span><span class="o">==</span>0.22.1 <span class="nv">torchaudio</span><span class="o">==</span>2.7.1 <span class="nt">--index-url</span> https://download.pytorch.org/whl/cu128
</code></pre></div> </div> </li> <li><strong>Log in to WandB &amp; Hugging Face</strong> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>wandb login
</code></pre></div> </div> <p>(Tip: If your school email can be verified, you can use WandB for free — even after graduation.)</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>huggingface-cli login
</code></pre></div> </div> </li> <li><strong>Install Additional Required Libraries</strong> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>transformers iniconfig pytest
</code></pre></div> </div> </li> <li>Depending on your environment, you might get an error asking you to install ffmpeg. There are several ways to install it, but the following method worked most cleanly: <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda <span class="nb">install</span> <span class="nt">-n</span> pi0 <span class="nt">-c</span> conda-forge <span class="s2">"ffmpeg&gt;=6,&lt;8"</span>
</code></pre></div> </div> <p>👉 This installs ffmpeg (version ≥6 and &lt;8) into the conda environment named pi0.</p> </li> <li><strong>Request Access to the Pretrained Model (PaliGemma)</strong> Get access here: 🔗 https://huggingface.co/google/paligemma-3b-pt-224</li> </ul> <h2 id="pi0-finetuning">Pi0 finetuning</h2> <p>When you install LeRobot, it automatically links the command lerobot-train to train.py. However, you can also run it directly like this. Since the command is quite long, it’s convenient to save it as a train.sh file and run it using:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sh train.sh
</code></pre></div></div> <ul> <li>Example training command: <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span>3 python src/lerobot/scripts/train.py <span class="se">\</span>
<span class="nt">--policy</span>.path<span class="o">=</span>lerobot/pi0 <span class="se">\</span>
<span class="nt">--dataset</span>.repo_id<span class="o">={</span>HF_USER<span class="o">}</span>/<span class="o">{</span>data_name<span class="o">}</span> <span class="se">\</span>
<span class="nt">--output_dir</span><span class="o">=</span>outputs/train/<span class="o">{</span>project_name<span class="o">}</span> <span class="se">\</span>
<span class="nt">--job_name</span><span class="o">=</span>pi0_so101_finetune <span class="se">\</span>
<span class="nt">--policy</span>.device<span class="o">=</span>cuda <span class="se">\</span>
<span class="nt">--policy</span>.repo_id<span class="o">=</span>None <span class="se">\</span>
<span class="nt">--task</span><span class="o">=</span><span class="s2">"move doll into a cup"</span> <span class="se">\</span>
<span class="nt">--wandb</span>.enable<span class="o">=</span><span class="nb">true</span> <span class="se">\</span>
<span class="nt">--wandb</span>.project<span class="o">=</span><span class="s2">"{project_name}"</span>
</code></pre></div> </div> </li> <li> <p>Parameter explanations: policy.path – If you’re using pretrained model weights, specify the Hugging Face repository containing those weights.<br/> policy.repo_id – If you want to automatically upload your trained model to Hugging Face during training, specify your own repository here.<br/> task – I didn’t realize at first, but this is where you input the text prompt that serves as a language command for the model.<br/> wandb.enable – Set this to true to visualize your training logs as graphs on the WandB dashboard.<br/></p> </li> <li> <p>config setting: num_episode: 50 <br/> batch_size: 8 <br/> save_steps: 5000 / 10000 <br/> “freeze_vision_encoder”: true, <br/> “train_expert_only”: <strong>false</strong>, <br/> “train_state_proj”: true</p> </li> <li>Modifying Training Configuration: In src/lerobot/configs/train.py, you can adjust parameters such as:<br/> num_workers, batch_size, steps, eval_freq, log_freq, and save_freq.<br/> In src/lerobot/policies/pi0/configuration_pi0.py, you can modify additional options such as train_expert_only and related settings.</li> </ul> <h2 id="uploading-a-trained-model-to-hugging-face-specific-checkpoint">Uploading a Trained Model to Hugging Face (Specific Checkpoint)</h2> <p>To upload a specific checkpoint to Hugging Face, use the following command:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>huggingface-cli upload <span class="o">{</span>HF_USER<span class="o">}</span>/<span class="o">{</span>project_name<span class="o">}</span> outputs/train/<span class="o">{</span>project_name<span class="o">}</span>/checkpoints/<span class="o">{</span>checkpoint_step<span class="o">}</span>/pretrained_model
</code></pre></div></div> <p>There is no need to manually create the repository in advance — it will be created automatically as a public repository during upload.</p> <h2 id="inference">Inference</h2> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>lerobot-record  <span class="se">\</span>
<span class="nt">--robot</span>.type<span class="o">=</span>so100_follower <span class="se">\</span>
<span class="nt">--robot</span>.port<span class="o">=</span>/dev/ttyACM1 <span class="se">\</span>
<span class="nt">--robot</span>.cameras<span class="o">=</span><span class="s2">"{ up: {type: opencv, index_or_path: /dev/video10, width: 640, height: 480, fps: 30}, side: {type: intelrealsense, serial_number_or_name: 233522074606, width: 640, height: 480, fps: 30}}"</span> <span class="se">\</span>
<span class="nt">--robot</span>.id<span class="o">=</span>my_awesome_follower_arm <span class="se">\</span>
<span class="nt">--display_data</span><span class="o">=</span><span class="nb">false</span> <span class="se">\</span>
<span class="nt">--dataset</span>.repo_id<span class="o">=</span><span class="k">${</span><span class="nv">HF_USER</span><span class="k">}</span>/eval_so100 <span class="se">\</span>
<span class="nt">--dataset</span>.single_task<span class="o">=</span><span class="s2">"Put lego brick into the transparent box"</span> <span class="se">\</span>
<span class="c"># &lt;- Teleop optional if you want to teleoperate in between episodes \</span>
<span class="c"># -teleop.type=so100_leader \</span>
<span class="c"># -teleop.port=/dev/ttyACM0 \</span>
<span class="c"># -teleop.id=my_awesome_leader_arm \</span>
- <span class="nt">-policy</span>.path<span class="o">=</span><span class="k">${</span><span class="nv">HF_USER</span><span class="k">}</span>/my_policy
</code></pre></div></div> <h2 id="some-comments">Some comments</h2> <p>Unlike ACT or Diffusion Policy, which are trained from scratch, I fine-tuned Pi0 and ran inference with it. Even though the training data were collected only in bright indoor environments with the lights on, the robot still managed to perform tasks pretty accurately even in darker settings.</p> <p>From what I’ve seen in GitHub issues and tutorials, Pi0 tends to work best when it’s heavily overfitted to a small number of tasks — kind of the opposite of what you’d expect from a general foundation model. It seems that even large-scale robot foundation models struggle with the huge diversity in robot hardware, task types, and data collection environments.</p> <p>I’m really curious to see how GROOT handles this.</p> <p><br/> <br/> Feel free to reach out if you run into any problems or have suggestions for improving this setup!</p>]]></content><author><name></name></author><category term="robotics"/><category term="Imitation_leaning,"/><category term="LeRobot,"/><category term="Pi0"/><summary type="html"><![CDATA[This is a blog post on how to finetune LeRobot Pi0 with SO-ARM101.]]></summary></entry></feed>