<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://ghuijo.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://ghuijo.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-10-09T19:20:03+00:00</updated><id>https://ghuijo.github.io/feed.xml</id><title type="html">blank</title><entry><title type="html">LeRobot Pi0 Finetuning Tutorial</title><link href="https://ghuijo.github.io/blog/2025/LeRobot-PI0-Finetuning-Tutorial/" rel="alternate" type="text/html" title="LeRobot Pi0 Finetuning Tutorial"/><published>2025-08-29T00:00:00+00:00</published><updated>2025-08-29T00:00:00+00:00</updated><id>https://ghuijo.github.io/blog/2025/LeRobot-PI0-Finetuning-Tutorial</id><content type="html" xml:base="https://ghuijo.github.io/blog/2025/LeRobot-PI0-Finetuning-Tutorial/"><![CDATA[<p>This is a blog post on how to finetune LeRobot Pi0.</p> <h3 id="references">References</h3> <p><a href="https://huggingface.co/docs/lerobot/il_robots">LeRobot Imitation Learning Tutorial</a> <a href="https://huggingface.co/blog/pi0">LeRobot Pi0 and Pi0-fast Blog</a></p> <h3 id="experiment-environment">Experiment environment</h3> <ul> <li>GPU: Tesla A100 SXM4 40GB</li> <li>OS: Ubuntu 20.04 LTS</li> <li>NVIDIA Driver: 570.169 (CUDA 12.8)</li> </ul> <h3 id="anaconda-environment-setup">Anaconda environment setup</h3> <ul> <li> <p><strong>Install Python 3.10 or higher</strong></p> </li> <li><strong>Install PyTorch</strong> Currently, LeRobot only supports PyTorch versions below 2.8. Use the following command to install a compatible version: <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span><span class="nv">torch</span><span class="o">==</span>2.7.1 <span class="nv">torchvision</span><span class="o">==</span>0.22.1 <span class="nv">torchaudio</span><span class="o">==</span>2.7.1 <span class="nt">--index-url</span> https://download.pytorch.org/whl/cu128
</code></pre></div> </div> </li> <li><strong>Log in to WandB &amp; Hugging Face</strong> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>wandb login
</code></pre></div> </div> <p>(Tip: If your school email can be verified, you can use WandB for free ‚Äî even after graduation.)</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>huggingface-cli login
</code></pre></div> </div> </li> <li><strong>Install Additional Required Libraries</strong> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>transformers iniconfig pytest
</code></pre></div> </div> </li> </ul> <p>Depending on your environment, you might get an error asking you to install ffmpeg. There are several ways to install it, but the following method worked most cleanly:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda <span class="nb">install</span> <span class="nt">-n</span> pi0 <span class="nt">-c</span> conda-forge <span class="s2">"ffmpeg&gt;=6,&lt;8"</span>
</code></pre></div></div> <p>üëâ This installs ffmpeg (version ‚â•6 and &lt;8) into the conda environment named pi0.</p> <ul> <li><strong>Request Access to the Pretrained Model (PaliGemma)</strong> Get access here: üîó https://huggingface.co/google/paligemma-3b-pt-224</li> </ul> <h3 id="pi0-finetuning">Pi0 finetuning</h3> <p>When you install LeRobot, it automatically links the command lerobot-train to train.py. However, you can also run it directly like this. Since the command is quite long, it‚Äôs convenient to save it as a train.sh file and run it using:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sh train.sh
</code></pre></div></div> <ul> <li>Example training command: <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span>3 python src/lerobot/scripts/train.py <span class="se">\</span>
<span class="nt">--policy</span>.path<span class="o">=</span>lerobot/pi0 <span class="se">\</span>
<span class="nt">--dataset</span>.repo_id<span class="o">={</span>HF_USER<span class="o">}</span>/<span class="o">{</span>data_name<span class="o">}</span> <span class="se">\</span>
<span class="nt">--output_dir</span><span class="o">=</span>outputs/train/<span class="o">{</span>project_name<span class="o">}</span> <span class="se">\</span>
<span class="nt">--job_name</span><span class="o">=</span>pi0_so101_finetune <span class="se">\</span>
<span class="nt">--policy</span>.device<span class="o">=</span>cuda <span class="se">\</span>
<span class="nt">--policy</span>.repo_id<span class="o">=</span>None <span class="se">\</span>
<span class="nt">--task</span><span class="o">=</span><span class="s2">"move doll into a cup"</span> <span class="se">\</span>
<span class="nt">--wandb</span>.enable<span class="o">=</span><span class="nb">true</span> <span class="se">\</span>
<span class="nt">--wandb</span>.project<span class="o">=</span><span class="s2">"{project_name}"</span>
</code></pre></div> </div> </li> <li> <p>Parameter explanations: policy.path ‚Äì If you‚Äôre using pretrained model weights, specify the Hugging Face repository containing those weights. policy.repo_id ‚Äì If you want to automatically upload your trained model to Hugging Face during training, specify your own repository here. task ‚Äì I didn‚Äôt realize at first, but this is where you input the text prompt that serves as a language command for the model. wandb.enable ‚Äì Set this to true to visualize your training logs as graphs on the WandB dashboard.</p> </li> <li>config setting: num_episode: 50 batch_size: 8 save_steps: 5000 / 10000 ‚Äúfreeze_vision_encoder‚Äù: true, ‚Äútrain_expert_only‚Äù: <strong>false</strong>, ‚Äútrain_state_proj‚Äù: true</li> </ul> <h3 id="modifying-training-configuration">Modifying Training Configuration</h3> <p>In src/lerobot/configs/train.py, you can adjust parameters such as: num_workers, batch_size, steps, eval_freq, log_freq, and save_freq.</p> <p>In src/lerobot/policies/pi0/configuration_pi0.py, you can modify additional options such as train_expert_only and related settings.</p> <h3 id="uploading-a-trained-model-to-hugging-face-specific-checkpoint">Uploading a Trained Model to Hugging Face (Specific Checkpoint)</h3> <p>To upload a specific checkpoint to Hugging Face, use the following command:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>huggingface-cli upload <span class="o">{</span>HF_USER<span class="o">}</span>/<span class="o">{</span>project_name<span class="o">}</span> outputs/train/<span class="o">{</span>project_name<span class="o">}</span>/checkpoints/<span class="o">{</span>checkpoint_step<span class="o">}</span>/pretrained_model
</code></pre></div></div> <p>There is no need to manually create the repository in advance ‚Äî it will be created automatically as a public repository during upload.</p> <h3 id="inference">Inference</h3> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>lerobot-record  <span class="se">\</span>
<span class="nt">--robot</span>.type<span class="o">=</span>so100_follower <span class="se">\</span>
<span class="nt">--robot</span>.port<span class="o">=</span>/dev/ttyACM1 <span class="se">\</span>
<span class="nt">--robot</span>.cameras<span class="o">=</span><span class="s2">"{ up: {type: opencv, index_or_path: /dev/video10, width: 640, height: 480, fps: 30}, side: {type: intelrealsense, serial_number_or_name: 233522074606, width: 640, height: 480, fps: 30}}"</span> <span class="se">\</span>
<span class="nt">--robot</span>.id<span class="o">=</span>my_awesome_follower_arm <span class="se">\</span>
<span class="nt">--display_data</span><span class="o">=</span><span class="nb">false</span> <span class="se">\</span>
<span class="nt">--dataset</span>.repo_id<span class="o">=</span><span class="k">${</span><span class="nv">HF_USER</span><span class="k">}</span>/eval_so100 <span class="se">\</span>
<span class="nt">--dataset</span>.single_task<span class="o">=</span><span class="s2">"Put lego brick into the transparent box"</span> <span class="se">\</span>
<span class="c"># &lt;- Teleop optional if you want to teleoperate in between episodes \</span>
<span class="c"># -teleop.type=so100_leader \</span>
<span class="c"># -teleop.port=/dev/ttyACM0 \</span>
<span class="c"># -teleop.id=my_awesome_leader_arm \</span>
- <span class="nt">-policy</span>.path<span class="o">=</span><span class="k">${</span><span class="nv">HF_USER</span><span class="k">}</span>/my_policy
</code></pre></div></div> <p>Feel free to reach out if you run into any problems or have suggestions for improving this setup!</p>]]></content><author><name></name></author><category term="robotics"/><category term="Imitation_leaning,"/><category term="LeRobot,"/><category term="Pi0"/><summary type="html"><![CDATA[This is a blog post on how to finetune LeRobot Pi0.]]></summary></entry></feed>